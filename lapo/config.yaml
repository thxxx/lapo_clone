env_name: bigfish
exp_name: ~ # filled in by the launch.sh script (consistent across stages 1-3)
stage_exp_name: ~ # updated in stages 1-3, used for wandb logging as a suffix

# configure model architecture
model:
  wm_scale: 24
  idm_impala_scale: 4
  policy_impala_scale: 4
  vq:
    enabled: true
    num_codebooks: 2
    num_discrete_latents: 4
    emb_dim: 16
    num_embs: 64 # each of the vq_latent_dim latents is a categorical variable with vq_num_embs values
    commitment_cost: 0.05
    decay: 0.999

# stage-1 (latent idm & wm training) hyperparameters
stage1:
  lr: 3e-4
  bs: 128
  steps: 50_000

# stage-2 (latent behavior cloning) hyperparameters
stage2:
  lr: 2e-4
  bs: 128
  steps: 60_000

# stage-3 (rl-finetuning) hyperparameters
stage3:
  steps: 4_000_000
  num_envs: 64
  grad_accum_f: 1
  num_steps: 64
  num_minibatches: 8
  update_epochs: 3
  ent_coef: 0.0025
  lr: 1e-2
  anneal_lr: true
  norm_adv: true
  clip_coef: 0.2
  clip_vloss: true
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: ~
  gamma: 0.999
  gae_lambda: 0.95
